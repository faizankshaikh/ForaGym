{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65105345",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/faizankshaikh/ForaGym/blob/main/examples/trial1_foragymSimple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b364187",
   "metadata": {},
   "source": [
    "# 1. Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "WEiHNbct33VM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WEiHNbct33VM",
    "outputId": "750d83fa-5480-434d-d567-f8fe610a5cfb"
   },
   "outputs": [],
   "source": [
    "!git clone --quiet https://github.com/faizankshaikh/ForaGym.git\n",
    "%cd ForaGym\n",
    "!pip install -q -e .\n",
    "!pip install stable-baselines3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3456ed9",
   "metadata": {},
   "source": [
    "# 2. Create Gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7693b5ea-4e87-4149-aa8c-fa9d709e8c91",
   "metadata": {
    "id": "7693b5ea-4e87-4149-aa8c-fa9d709e8c91"
   },
   "outputs": [],
   "source": [
    "# import important libs and modules\n",
    "import gym\n",
    "import foragym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "314c8477-4e4c-40ac-a3b7-e26ac9f7d407",
   "metadata": {
    "id": "314c8477-4e4c-40ac-a3b7-e26ac9f7d407"
   },
   "outputs": [],
   "source": [
    "# initiatlize gym env\n",
    "env = gym.make(\"foragym:foragym/ForaGym-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d0a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize transition matrix\n",
    "# for key, items in env.P.items():\n",
    "#     try:\n",
    "#         days_left, life_point, field, weather = env.decode(key)\n",
    "#         p_wait, new_state_wait, reward_wait, is_dead_wait = items[0][0]\n",
    "#         p_forage_failure, new_state_forage_failure, reward_forage_failure, is_dead_forage_failure = items[1][0]\n",
    "\n",
    "#         print(\"=\"*10)\n",
    "#         print(f\"days_left: {days_left}, life_point: {life_point}, field: {field}, weather: {weather}, enc_state: {key}\")\n",
    "#         print(f\"p_wait: {p_wait:.2f}, new_state_wait: {new_state_wait}, reward_wait: {reward_wait}, is_dead_wait: {is_dead_wait}\")\n",
    "#         print(f\"p_forage_failure: {p_forage_failure:.2f}, new_state_forage_failure: {new_state_forage_failure}, reward_forage_failure: {reward_forage_failure}, is_dead_forage_failure: {is_dead_forage_failure}\")\n",
    "#         p_forage_success, new_state_forage_success, reward_forage_success, is_dead_forage_success = items[1][12]\n",
    "#         print(f\"p_forage_success: {p_forage_success:.2f}, new_state_forage_success: {new_state_forage_success}, reward_forage_success: {reward_forage_success}, is_dead_forage_success: {is_dead_forage_success}\")\n",
    "#         print(\"=\"*10)\n",
    "#     except:\n",
    "#         pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6890be27",
   "metadata": {},
   "source": [
    "# 3. Solve the foraging task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7677b8",
   "metadata": {},
   "source": [
    "## 3.1 Heuristic method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94752668-8c12-4795-93e7-7a80dd8c1342",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94752668-8c12-4795-93e7-7a80dd8c1342",
    "outputId": "2b042364-fb9b-47d3-ce35-54235e35dbb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode #1\n",
      "==========\n",
      "Initial state:-\n",
      "--Days left: 5\n",
      "--State of Field: [0 0 0 0 1]\n",
      "--Current life: 6\n",
      "--Type of Weather: Rainy\n",
      "\n",
      "Action to take: Wait\n",
      "Chance to find food: 0.00\n",
      "Reward?: 0\n",
      "is Dead?: False\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 4\n",
      "--State of Field: [0 0 1 0 1]\n",
      "--Current life: 5\n",
      "--Type of Weather: Rainy\n",
      "\n",
      "Action to take: Wait\n",
      "Chance to find food: 0.00\n",
      "Reward?: 0\n",
      "is Dead?: False\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 3\n",
      "--State of Field: [0 1 1 0 0]\n",
      "--Current life: 4\n",
      "--Type of Weather: Clear\n",
      "\n",
      "Action to take: Wait\n",
      "Chance to find food: 0.00\n",
      "Reward?: 0\n",
      "is Dead?: False\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 2\n",
      "--State of Field: [0 1 1 1 0]\n",
      "--Current life: 3\n",
      "--Type of Weather: Clear\n",
      "\n",
      "Action to take: Wait\n",
      "Chance to find food: 0.00\n",
      "Reward?: 0\n",
      "is Dead?: False\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 1\n",
      "--State of Field: [0 0 1 0 1]\n",
      "--Current life: 2\n",
      "--Type of Weather: Rainy\n",
      "\n",
      "Action to take: Wait\n",
      "Chance to find food: 0.00\n",
      "Reward?: 0\n",
      "is Dead?: True\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 1\n",
      "--State of Field: [1 1 0 1 1]\n",
      "--Current life: 2\n",
      "--Type of Weather: Rainy\n",
      "\n",
      "----------\n",
      "Episode #2\n",
      "==========\n",
      "Initial state:-\n",
      "--Days left: 5\n",
      "--State of Field: [1 1 0 0 1]\n",
      "--Current life: 5\n",
      "--Type of Weather: Clear\n",
      "\n",
      "Action to take: Wait\n",
      "Chance to find food: 0.00\n",
      "Reward?: 0\n",
      "is Dead?: False\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 4\n",
      "--State of Field: [0 1 1 1 1]\n",
      "--Current life: 4\n",
      "--Type of Weather: Rainy\n",
      "\n",
      "Action to take: Wait\n",
      "Chance to find food: 0.00\n",
      "Reward?: 0\n",
      "is Dead?: False\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 3\n",
      "--State of Field: [1 1 1 0 1]\n",
      "--Current life: 3\n",
      "--Type of Weather: Rainy\n",
      "\n",
      "Action to take: Wait\n",
      "Chance to find food: 0.00\n",
      "Reward?: 0\n",
      "is Dead?: False\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 2\n",
      "--State of Field: [0 1 0 1 0]\n",
      "--Current life: 2\n",
      "--Type of Weather: Clear\n",
      "\n",
      "Action to take: Wait\n",
      "Chance to find food: 0.00\n",
      "Reward?: 0\n",
      "is Dead?: False\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 1\n",
      "--State of Field: [0 0 0 0 1]\n",
      "--Current life: 1\n",
      "--Type of Weather: Rainy\n",
      "\n",
      "Action to take: Wait\n",
      "Chance to find food: 0.00\n",
      "Reward?: -1\n",
      "is Dead?: True\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 1\n",
      "--State of Field: [0 1 1 0 1]\n",
      "--Current life: 1\n",
      "--Type of Weather: Rainy\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# run algorithm on sample episodes\n",
    "num_episodes = 2\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    print(f\"Episode #{episode+1}\")\n",
    "    print(\"=\" * 10)\n",
    "\n",
    "    is_dead = False\n",
    "    obs = env.reset()\n",
    "\n",
    "    print(\"Initial state:-\")\n",
    "    env.render()\n",
    "    print()\n",
    "\n",
    "    while not is_dead:\n",
    "        action = 0 #env.action_space.sample()\n",
    "        print(f\"Action to take: {env.ACTION_DICT[action]}\")\n",
    "        obs, reward, is_dead, info = env.step(action)\n",
    "        print(f'Chance to find food: {info[\"chance\"]:.2f}')\n",
    "        print(f\"Reward?: {reward}\")\n",
    "        print(f\"is Dead?: {is_dead}\")\n",
    "        print()\n",
    "        print(\"Current game state:-\")\n",
    "        env.render()\n",
    "        print()\n",
    "\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4797aec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-0.83 +/- 0.37\n"
     ]
    }
   ],
   "source": [
    "# evaluate algorithm\n",
    "num_episodes = 1000\n",
    "episode_reward = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = 0 #env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "    episode_reward.append(total_reward)\n",
    "\n",
    "print(f\"mean_reward:{np.mean(episode_reward):.2f} +/- {np.std(episode_reward):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba0fd72",
   "metadata": {},
   "source": [
    "## 3.2 Value Iteration method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eb731c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal policy\n",
    "num_states = env.NUM_STATES\n",
    "num_actions = env.NUM_ACTIONS\n",
    "\n",
    "V = np.zeros(num_states)\n",
    "V[:95] = -1\n",
    "\n",
    "optimal_policy = np.zeros([num_states, num_actions])\n",
    "\n",
    "\n",
    "for days_left in range(1, env.NUM_DAYS_LEFT):\n",
    "\tfor life_point in range(1, env.NUM_LIFE_POINTS):\n",
    "\t\tfor field in range(env.NUM_FIELDS + 1):\n",
    "\t\t\tfor weather in range(env.NUM_WEATHER_TYPES):\n",
    "\t\t\t\tenc_state = env.encode(days_left, life_point, field, weather)\n",
    "\t\t\t\tactions_s = np.zeros(num_actions)\n",
    "\n",
    "\t\t\t\tfor action in range(env.NUM_ACTIONS):\n",
    "\t\t\t\t\tfor transition_prob, next_state, reward, done in env.P[enc_state][action]:\n",
    "\t\t\t\t\t\tactions_s[action] += reward + transition_prob * V[next_state]\n",
    "\t\t\t\tV[enc_state] = actions_s.max()\n",
    "\t\t\t\tif actions_s[0] != actions_s[1]:\n",
    "\t\t\t\t\toptimal_policy[enc_state, np.argmax(actions_s)] = 1.0\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\toptimal_policy[enc_state, 1] = 1.0\n",
    "\n",
    "\t\t\t\t# print(f\"days_left: {days_left}, life_point: {life_point}, field: {field}/{env.NUM_FIELDS}, weather: {env.WEATHER_DICT[weather]}:- action: {env.ACTION_DICT[np.argmax(optimal_policy[enc_state])]}, value: {V[enc_state]:.2f}\")\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "112304b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode #1\n",
      "==========\n",
      "Initial state:-\n",
      "--Days left: 5\n",
      "--State of Field: [0 1 0 1 1]\n",
      "--Current life: 5\n",
      "--Type of Weather: Rainy\n",
      "\n",
      "Action to take: Forage\n",
      "Chance to find food: 0.51\n",
      "Reward?: 0\n",
      "is Dead?: False\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 4\n",
      "--State of Field: [0 0 1 1 1]\n",
      "--Current life: 6\n",
      "--Type of Weather: Clear\n",
      "\n",
      "Action to take: Forage\n",
      "Chance to find food: 0.08\n",
      "Reward?: 0\n",
      "is Dead?: False\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 3\n",
      "--State of Field: [0 0 0 1 0]\n",
      "--Current life: 4\n",
      "--Type of Weather: Clear\n",
      "\n",
      "Action to take: Wait\n",
      "Chance to find food: 0.00\n",
      "Reward?: 0\n",
      "is Dead?: False\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 2\n",
      "--State of Field: [1 1 1 0 1]\n",
      "--Current life: 3\n",
      "--Type of Weather: Clear\n",
      "\n",
      "Action to take: Wait\n",
      "Chance to find food: 0.00\n",
      "Reward?: 0\n",
      "is Dead?: False\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 1\n",
      "--State of Field: [1 0 1 1 1]\n",
      "--Current life: 2\n",
      "--Type of Weather: Rainy\n",
      "\n",
      "Action to take: Wait\n",
      "Chance to find food: 0.00\n",
      "Reward?: 0\n",
      "is Dead?: True\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 1\n",
      "--State of Field: [1 1 0 0 0]\n",
      "--Current life: 2\n",
      "--Type of Weather: Clear\n",
      "\n",
      "----------\n",
      "Episode #2\n",
      "==========\n",
      "Initial state:-\n",
      "--Days left: 5\n",
      "--State of Field: [0 0 0 0 1]\n",
      "--Current life: 6\n",
      "--Type of Weather: Clear\n",
      "\n",
      "Action to take: Wait\n",
      "Chance to find food: 0.00\n",
      "Reward?: 0\n",
      "is Dead?: False\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 4\n",
      "--State of Field: [1 1 1 0 0]\n",
      "--Current life: 5\n",
      "--Type of Weather: Rainy\n",
      "\n",
      "Action to take: Wait\n",
      "Chance to find food: 0.00\n",
      "Reward?: 0\n",
      "is Dead?: False\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 3\n",
      "--State of Field: [1 1 0 1 1]\n",
      "--Current life: 4\n",
      "--Type of Weather: Rainy\n",
      "\n",
      "Action to take: Wait\n",
      "Chance to find food: 0.00\n",
      "Reward?: 0\n",
      "is Dead?: False\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 2\n",
      "--State of Field: [0 1 0 0 0]\n",
      "--Current life: 3\n",
      "--Type of Weather: Rainy\n",
      "\n",
      "Action to take: Wait\n",
      "Chance to find food: 0.00\n",
      "Reward?: 0\n",
      "is Dead?: False\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 1\n",
      "--State of Field: [0 1 0 1 1]\n",
      "--Current life: 2\n",
      "--Type of Weather: Clear\n",
      "\n",
      "Action to take: Wait\n",
      "Chance to find food: 0.00\n",
      "Reward?: 0\n",
      "is Dead?: True\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 1\n",
      "--State of Field: [0 1 0 0 0]\n",
      "--Current life: 2\n",
      "--Type of Weather: Rainy\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# run algorithm on sample episodes\n",
    "num_episodes = 2\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    print(f\"Episode #{episode+1}\")\n",
    "    print(\"=\" * 10)\n",
    "\n",
    "    is_dead = False\n",
    "    obs = env.reset()\n",
    "\n",
    "    print(\"Initial state:-\")\n",
    "    env.render()\n",
    "    print()\n",
    "\n",
    "    while not is_dead:\n",
    "        enc_state = env.encode(obs[\"days_left\"], obs[\"life_points\"], sum(obs[\"field_state\"]), obs[\"weather_type\"])\n",
    "        action = np.argmax(optimal_policy[enc_state])\n",
    "        print(f\"Action to take: {env.ACTION_DICT[action]}\")\n",
    "        obs, reward, is_dead, info = env.step(action)\n",
    "        print(f'Chance to find food: {info[\"chance\"]:.2f}')\n",
    "        print(f\"Reward?: {reward}\")\n",
    "        print(f\"is Dead?: {is_dead}\")\n",
    "        print()\n",
    "        print(\"Current game state:-\")\n",
    "        env.render()\n",
    "        print()\n",
    "\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f20d1d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-0.59 +/- 0.49\n"
     ]
    }
   ],
   "source": [
    "# evaluate algorithm\n",
    "num_episodes = 1000\n",
    "episode_reward = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        enc_state = env.encode(obs[\"days_left\"], obs[\"life_points\"], sum(obs[\"field_state\"]), obs[\"weather_type\"])\n",
    "        action = np.argmax(optimal_policy[enc_state])\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "    episode_reward.append(total_reward)\n",
    "\n",
    "print(f\"mean_reward:{np.mean(episode_reward):.2f} +/- {np.std(episode_reward):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f20b4",
   "metadata": {},
   "source": [
    "## 3.3 DQN (Stable baselines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "268ac3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and train\n",
    "model = DQN('MultiInputPolicy', env, learning_starts=1e5, gamma=0.9, exploration_fraction=0.3).learn(total_timesteps=int(2e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c95d07e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode #1\n",
      "==========\n",
      "Initial state:-\n",
      "--Days left: 5\n",
      "--State of Field: [0 0 0 1 0]\n",
      "--Current life: 1\n",
      "--Type of Weather: Clear\n",
      "\n",
      "Action to take: Forage\n",
      "Chance to find food: 0.12\n",
      "Reward?: -1\n",
      "is Dead?: True\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 5\n",
      "--State of Field: [1 1 0 0 1]\n",
      "--Current life: 1\n",
      "--Type of Weather: Clear\n",
      "\n",
      "----------\n",
      "Episode #2\n",
      "==========\n",
      "Initial state:-\n",
      "--Days left: 5\n",
      "--State of Field: [0 0 0 0 1]\n",
      "--Current life: 4\n",
      "--Type of Weather: Rainy\n",
      "\n",
      "Action to take: Forage\n",
      "Chance to find food: 0.41\n",
      "Reward?: 0\n",
      "is Dead?: False\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 4\n",
      "--State of Field: [0 0 1 1 1]\n",
      "--Current life: 2\n",
      "--Type of Weather: Rainy\n",
      "\n",
      "Action to take: Wait\n",
      "Chance to find food: 0.00\n",
      "Reward?: 0\n",
      "is Dead?: False\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 3\n",
      "--State of Field: [1 1 1 1 1]\n",
      "--Current life: 1\n",
      "--Type of Weather: Clear\n",
      "\n",
      "Action to take: Forage\n",
      "Chance to find food: 0.75\n",
      "Reward?: -1\n",
      "is Dead?: True\n",
      "\n",
      "Current game state:-\n",
      "--Days left: 3\n",
      "--State of Field: [0 0 0 0 1]\n",
      "--Current life: 1\n",
      "--Type of Weather: Clear\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# run algorithm on sample episodes\n",
    "num_episodes = 2\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    print(f\"Episode #{episode+1}\")\n",
    "    print(\"=\" * 10)\n",
    "\n",
    "    is_dead = False\n",
    "    obs = env.reset()\n",
    "\n",
    "    print(\"Initial state:-\")\n",
    "    env.render()\n",
    "    print()\n",
    "\n",
    "    while not is_dead:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        print(f\"Action to take: {env.ACTION_DICT[action]}\")\n",
    "        obs, reward, is_dead, info = env.step(action)\n",
    "        print(f'Chance to find food: {info[\"chance\"]:.2f}')\n",
    "        print(f\"Reward?: {reward}\")\n",
    "        print(f\"is Dead?: {is_dead}\")\n",
    "        print()\n",
    "        print(\"Current game state:-\")\n",
    "        env.render()\n",
    "        print()\n",
    "\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ded4a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize policy\n",
    "# for idx in range(num_states):\n",
    "#     days_left, life_point, field, weather = env.decode(idx)\n",
    "#     field_state = np.zeros(env.NUM_FIELDS, dtype=int)\n",
    "#     field_state[:field] = 1\n",
    "#     obs = {\n",
    "#         \"days_left\": days_left,\n",
    "#         \"life_points\": life_point,\n",
    "#         \"field_state\": field_state,\n",
    "#         \"weather_type\": weather\n",
    "#     }\n",
    "\n",
    "#     action, _ = model.predict(obs, deterministic=True)\n",
    "#     print(f\"days_left: {days_left}, life_point: {life_point}, field: {field}/{env.NUM_FIELDS}, weather: {env.WEATHER_DICT[weather]}:- action: {env.ACTION_DICT[action]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "313e4549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-0.48 +/- 0.50\n"
     ]
    }
   ],
   "source": [
    "# evaluate algorithm\n",
    "num_episodes = 1000\n",
    "episode_reward = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "    episode_reward.append(total_reward)\n",
    "\n",
    "print(f\"mean_reward:{np.mean(episode_reward):.2f} +/- {np.std(episode_reward):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770594f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "8bc7b30897903e88fa3d4eb0381d863b99bc16a056ca478157cef09ec2d1b978"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
